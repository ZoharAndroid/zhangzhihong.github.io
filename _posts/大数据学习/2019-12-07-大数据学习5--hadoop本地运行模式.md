---
layout: post
title: hadoop本地运行模式
tags: [大数据]
categories: 大数据
---

经过前面几篇博客的介绍，接下来开始正式进入hadoop的学习。首先是hadoop的单节集群学习。参考文档：https://hadoop.apache.org/docs/r2.10.0/hadoop-project-dist/hadoop-common/SingleCluster.html 。

<!-- TOC -->

- [1. 到hadoop目录下编辑文件](#1-%e5%88%b0hadoop%e7%9b%ae%e5%bd%95%e4%b8%8b%e7%bc%96%e8%be%91%e6%96%87%e4%bb%b6)
- [2. 本地模式操作](#2-%e6%9c%ac%e5%9c%b0%e6%a8%a1%e5%bc%8f%e6%93%8d%e4%bd%9c)
- [3. wordcount例子](#3-wordcount%e4%be%8b%e5%ad%90)

<!-- /TOC -->

# 1. 到hadoop目录下编辑文件

切换道hadoop的安装文件下，然后编辑`etc/hadoop/hadoop-env.sh`。
```
sudo vim etc/hadoop/hadoop-env.sh
```

找到该文件中的**JAVA_HOME**，然后修改JAVA_HOME后面的值，如下所示：
```
export JAVA_HOME=/opt/module/jdk1.8.0_161
```
保存退出。

输入下面的命令看看是否输出：hadoop脚本的使用文档。
```
[zohar@hadoop100 hadoop-2.10.0]$ bin/hadoop
Usage: hadoop [--config confdir] [COMMAND | CLASSNAME]
  CLASSNAME            run the class named CLASSNAME
 or
  where COMMAND is one of:
  fs                   run a generic filesystem user client
  version              print the version
  jar <jar>            run a jar file
                       note: please use "yarn jar" to launch
                             YARN applications, not this command.
  checknative [-a|-h]  check native hadoop and compression libraries availability
  distcp <srcurl> <desturl> copy file or directories recursively
  archive -archiveName NAME -p <parent path> <src>* <dest> create a hadoop archive
  classpath            prints the class path needed to get the
                       Hadoop jar and the required libraries
  credential           interact with credential providers
  daemonlog            get/set the log level for each daemon
  trace                view and modify Hadoop tracing settings

Most commands print help when invoked w/o parameters.
```
> 可以看到输出了hadoop脚本说明，说明我这里是配置好了。这里需要注意的是，这里需要切换到hadoop的安装目录下。如果没有，请输入：`cd /opt/module/hadoop-2.10.0`。

# 2. 本地模式操作

依次输入下面的命令：
```
 $ mkdir input
  $ cp etc/hadoop/*.xml input
  $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.10.0.jar grep input output 'dfs[a-z.]+'
  $ cat output/*
  ```

  输出：
  ```
1	dfsadmin
```

可以看到运行成功了。

# 3. wordcount例子

下面再看一个wordcount例子：看看统计单词的数目。

创建wcinput目录：
```
mkdir wcinput
```

给文件输入内容：
```
vim wc.input
```
然后我这里输入的是：
```
zhangzhihong hadoop helloworld zhangzhihong a this a haha
```
> 这里随便写，这个例子就是统计单词重复出现的次数。

运行命令：
```
[root@hadoop100 hadoop-2.10.0]# bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.10.0.jar wordcount wc.input wcoutput
```

输出：
```
[root@hadoop100 hadoop-2.10.0]# cat wcoutput/*
a	2
hadoop	1
haha	1
helloworld	1
this	1
zhangzhihong	2
```

> 可以发现，这个统计是可以运行的。

