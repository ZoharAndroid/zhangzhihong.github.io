---
layout: post
title: 大数据学习8--集群分发方法学习
tags: [大数据]
categories: 大数据
---

这一篇博客主要是介绍一些集群之间相互传输内容的一些方法和脚本，所以这篇博客还是非常具有实用价值的。

<!-- TOC -->

- [1. scp： 安全拷贝](#1-scp-%e5%ae%89%e5%85%a8%e6%8b%b7%e8%b4%9d)
- [2. rsync](#2-rsync)
- [3. 使用脚本](#3-%e4%bd%bf%e7%94%a8%e8%84%9a%e6%9c%ac)

<!-- /TOC -->

# 1. scp： 安全拷贝

scp可以实现服务器与服务器之间的数据拷贝

```
[root@hadoop102 ~]# scp -r hadoop100:/opt/module/* hadoop102:/opt/module
```
> 这里要确保hadoop102有这个目录，否则会提示找不到这个目录。
> -r： 递归复制整个目录。

# 2. rsync

```
[root@hadoop103 module]# rsync -av hadoop100:/opt/module/* /opt/module/
```

scp和rsync两者的区别：
1. scp对于目的地址需要指定主机名，这一就意味着可以在任意一台主机上操作其他主机；rsync则不行，只能针对本主机。
2. rsync速度更快；rsync只对差异文件进行更新，scp全部都会复制过去。

# 3. 使用脚本

```
[zohar@hadoop100 ~]$ vim xsync 
```

```shell
#!/bin/bash
# 1. 获取输入参数的个数，如果没有参数，直接退出
pcount=$#
if((pcount==0)); then
echo no args;
exit;
fi

# 2. 获取文件名称
p1=$1
fname=`basename $p1`
echo fname=$fname

# 3. 获取上级目录到绝对路径
pdir=`cd -P $(dirname $p1);pwd`
echo pdir=$pdir

# 4. 获取当前用户名称
user=`whoami`

# 5. 循环
for((host=102;host<105;host++));do
        echo ----------------hadoop$host-------------------
        rsync -rvl $pdir/$fname $user@hadoop$host:$pdir
done
```

运行

```
[root@hadoop100 module]# ./xsync hadoop-2.10.0/
```
> 这里可以吧xsync复制到/bin目录下，这样就可以直接输入xsync命令了，而不用指明全路径了。`cp xysnc /bin`



